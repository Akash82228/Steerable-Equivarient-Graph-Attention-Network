{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QM9 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installation of necessary dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install torch geometric\n",
    "# %pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n",
    "# %pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-1.10.0+cu111.html\n",
    "# %pip install torch-geometric\n",
    "# %pip install -q git+https://github.com/snap-stanford/deepsnap.git\n",
    "# %pip install wget"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download QM9 dataset\n",
    "\n",
    "make dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Callable, List\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import os.path as osp\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_scatter import scatter\n",
    "from torch_geometric.data import (InMemoryDataset, download_url, extract_zip,\n",
    "                                  Data)\n",
    "from torch_geometric.nn import radius_graph\n",
    "from e3nn.o3 import Irreps, spherical_harmonics\n",
    "\n",
    "from torch_geometric.nn import TransformerConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "HAR2EV = 27.211386246\n",
    "KCALMOL2EV = 0.04336414\n",
    "\n",
    "conversion = torch.tensor([\n",
    "    1., 1., HAR2EV, HAR2EV, HAR2EV, 1., HAR2EV, HAR2EV, HAR2EV, HAR2EV, HAR2EV,\n",
    "    1., KCALMOL2EV, KCALMOL2EV, KCALMOL2EV, KCALMOL2EV, 1., 1., 1.\n",
    "])\n",
    "\n",
    "atomrefs = {\n",
    "    6: [0., 0., 0., 0., 0.],\n",
    "    7: [\n",
    "        -13.61312172, -1029.86312267, -1485.30251237, -2042.61123593,\n",
    "        -2713.48485589\n",
    "    ],\n",
    "    8: [\n",
    "        -13.5745904, -1029.82456413, -1485.26398105, -2042.5727046,\n",
    "        -2713.44632457\n",
    "    ],\n",
    "    9: [\n",
    "        -13.54887564, -1029.79887659, -1485.2382935, -2042.54701705,\n",
    "        -2713.42063702\n",
    "    ],\n",
    "    10: [\n",
    "        -13.90303183, -1030.25891228, -1485.71166277, -2043.01812778,\n",
    "        -2713.88796536\n",
    "    ],\n",
    "    11: [0., 0., 0., 0., 0.],\n",
    "}\n",
    "\n",
    "\n",
    "targets = ['mu', 'alpha', 'homo', 'lumo', 'gap', 'r2', 'zpve', 'U0',\n",
    "           'U', 'H', 'G', 'Cv', 'U0_atom', 'U_atom', 'H_atom', 'G_atom', 'A', 'B', 'C']\n",
    "\n",
    "tid = targets.index(\"alpha\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model run parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs =10\n",
    "batch_size=128\n",
    "lr = 5e-4 #learning rate\n",
    "weight_decay=1e-8\n",
    "num_workers =4 #num workers in dataloader\n",
    "save_dir = \"saved models\" #directory to save models\n",
    "\n",
    "#set the task variable\n",
    "task=\"graph\"\n",
    "\n",
    "#set the logging flag\n",
    "log = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"qm9\"\n",
    "root = \"datasets\" #dataset location\n",
    "target = \"alpha\" \n",
    "radius = 2 #Radius (Angstrom) between which atoms to add links.\n",
    "feature_type = \"one_hot\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =\"segnn\"\n",
    "hidden_features= 32 #number of hidden features\n",
    "lmax_h =2 #max degree of hidden rep\n",
    "lmax_attr=3 #max degree of geometric attribute embedding\n",
    "subspace_type = \"weightbalanced\" #how to divide spherical harmonic subspace\n",
    "layers = 3 #number of message passing layers\n",
    "norm = \"instance\" #normalisation type\n",
    "pool = \"avg\"\n",
    "conv_type = \"linear\" #linear or non-linear aggregation of local information in SEConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TargetGetter(object):\n",
    "    \"\"\" Gets relevant target \"\"\"\n",
    "\n",
    "    def __init__(self, target):\n",
    "        self.target = target\n",
    "        self.target_idx = targets.index(target)\n",
    "\n",
    "    def __call__(self, data):\n",
    "        # Specify target.\n",
    "        data.y = data.y[0, self.target_idx]\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QM9(InMemoryDataset):\n",
    "    \n",
    "    r\"\"\"The QM9 dataset from the `\"MoleculeNet: A Benchmark for Molecular\n",
    "    Machine Learning\" <https://arxiv.org/abs/1703.00564>`_ paper, consisting of\n",
    "    about 130,000 molecules with 19 regression targets.\n",
    "    Each molecule includes complete spatial information for the single low\n",
    "    energy conformation of the atoms in the molecule.\n",
    "    In addition, we provide the atom features from the `\"Neural Message\n",
    "    Passing for Quantum Chemistry\" <https://arxiv.org/abs/1704.01212>`_ paper. \"\"\"\n",
    "\n",
    "    raw_url = ('https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/'\n",
    "               'molnet_publish/qm9.zip')\n",
    "    raw_url2 = 'https://ndownloader.figshare.com/files/3195404'\n",
    "    processed_url = 'https://data.pyg.org/datasets/qm9_v3.zip'\n",
    "\n",
    "    def __init__(self, root, target, radius, partition, lmax_attr, feature_type):\n",
    "        # assert feature_type in [\"one_hot\", \"cormorant\", \"gilmer\"], \"Please use valid features\"\n",
    "        # assert target in targets\n",
    "        # assert partition in [\"train\", \"valid\", \"test\"]\n",
    "        self.root = osp.abspath(osp.join(root, \"qm9\"))\n",
    "        self.target = target\n",
    "        self.radius = radius\n",
    "        self.partition = partition\n",
    "        self.feature_type = feature_type\n",
    "        self.lmax_attr = lmax_attr\n",
    "        self.attr_irreps = Irreps.spherical_harmonics(lmax_attr)\n",
    "        transform = TargetGetter(self.target)\n",
    "\n",
    "        super().__init__(self.root, transform)\n",
    "\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    def calc_stats(self):\n",
    "        ys = np.array([data.y.item() for data in self])\n",
    "        mean = np.mean(ys)\n",
    "        mad = np.mean(np.abs(ys - mean))\n",
    "        return mean, mad\n",
    "\n",
    "    def atomref(self, target) -> Optional[torch.Tensor]:\n",
    "        if target in atomrefs:\n",
    "            out = torch.zeros(100)\n",
    "            out[torch.tensor([1, 6, 7, 8, 9])] = torch.tensor(atomrefs[target])\n",
    "            return out.view(-1, 1)\n",
    "        return None\n",
    "    \n",
    "    @ property\n",
    "    def raw_file_names(self) -> List[str]:\n",
    "        try:\n",
    "            import rdkit  # noqa\n",
    "            return ['gdb9.sdf', 'gdb9.sdf.csv', 'uncharacterized.txt']\n",
    "        except ImportError:\n",
    "            print(\"Please install rdkit\")\n",
    "            return\n",
    "        \n",
    "    @ property\n",
    "    def processed_file_names(self) -> str:\n",
    "        return [\"_\".join([self.partition, \"r=\"+str(np.round(self.radius, 2)),\n",
    "                          self.feature_type, \"l=\"+str(self.lmax_attr)]) + '.pt']\n",
    "    \n",
    "    def download(self):\n",
    "        print(\"Downloading to\", self.raw_dir, self.raw_url)\n",
    "        try:\n",
    "            import rdkit  # noqa\n",
    "            file_path = download_url(self.raw_url, self.raw_dir)\n",
    "            extract_zip(file_path, self.raw_dir)\n",
    "            os.unlink(file_path)\n",
    "\n",
    "            file_path = download_url(self.raw_url2, self.raw_dir)\n",
    "            os.rename(osp.join(self.raw_dir, '3195404'),\n",
    "                      osp.join(self.raw_dir, 'uncharacterized.txt'))\n",
    "        except ImportError:\n",
    "            path = download_url(self.processed_url, self.raw_dir)\n",
    "            extract_zip(path, self.raw_dir)\n",
    "            os.unlink(path)\n",
    "\n",
    "    '''This code defines the process() method of a PyTorch dataset class. \n",
    "    The purpose of this method is to process the raw data of the dataset into a format \n",
    "    that can be used for training a machine learning model.'''\n",
    "\n",
    "    def process(self):\n",
    "        try:\n",
    "            import rdkit\n",
    "            from rdkit import Chem\n",
    "            from rdkit.Chem.rdchem import HybridizationType\n",
    "            from rdkit.Chem.rdchem import BondType as BT\n",
    "            from rdkit import RDLogger\n",
    "            RDLogger.DisableLog('rdApp.*')\n",
    "        except ImportError:\n",
    "            print(\"Please install rdkit\")\n",
    "            return\n",
    "\n",
    "        print(\"Processing data for\", self.partition, \"set with radius=\" + str(np.round(self.radius, 2)) +\n",
    "              \",\", \"l_attr=\" + str(self.lmax_attr), \"and\", self.feature_type, \"features.\")\n",
    "        types = {'H': 0, 'C': 1, 'N': 2, 'O': 3, 'F': 4}\n",
    "\n",
    "        with open(self.raw_paths[1], 'r') as f:\n",
    "            target = f.read().split('\\n')[1:-1]\n",
    "            target = [[float(x) for x in line.split(',')[1:20]]\n",
    "                      for line in target]\n",
    "            target = torch.tensor(target, dtype=torch.float)\n",
    "            target = torch.cat([target[:, 3:], target[:, :3]], dim=-1)\n",
    "            target = target * conversion.view(1, -1)\n",
    "\n",
    "        with open(self.raw_paths[2], 'r') as f:\n",
    "            skip = [int(x.split()[0]) - 1 for x in f.read().split('\\n')[9:-2]]\n",
    "\n",
    "        suppl = Chem.SDMolSupplier(self.raw_paths[0], removeHs=False,\n",
    "                                   sanitize=False)\n",
    "        data_list = []\n",
    "\n",
    "        # Create splits \n",
    "        Nmols = len(suppl) - len(skip)\n",
    "        Ntrain = 100000\n",
    "        Ntest = int(0.1*Nmols)\n",
    "        Nvalid = Nmols - (Ntrain + Ntest)\n",
    "\n",
    "        # sets the random seed to a specific value (0), \n",
    "        # so that the same random permutation is generated every time the code is run.\n",
    "        np.random.seed(0) \n",
    "        data_perm = np.random.permutation(Nmols)\n",
    "        train, valid, test = np.split(data_perm, [Ntrain, Ntrain+Nvalid])\n",
    "        indices = {\"train\": train, \"valid\": valid, \"test\": test}\n",
    "\n",
    "        # Add a very ugly second index to align with Cormorant splits.\n",
    "        j = 0\n",
    "        for i, mol in enumerate(tqdm(suppl)):\n",
    "            if i in skip:\n",
    "                continue\n",
    "            if j not in indices[self.partition]:\n",
    "                j += 1\n",
    "                continue\n",
    "            j += 1\n",
    "\n",
    "            N = mol.GetNumAtoms()\n",
    "\n",
    "            pos = suppl.GetItemText(i).split('\\n')[4:4 + N]\n",
    "            pos = [[float(x) for x in line.split()[:3]] for line in pos]\n",
    "            pos = torch.tensor(pos, dtype=torch.float)\n",
    "\n",
    "            edge_index = radius_graph(pos, r=self.radius, loop=False)\n",
    "\n",
    "            type_idx = []\n",
    "            atomic_number = []\n",
    "            aromatic = []\n",
    "            sp = []\n",
    "            sp2 = []\n",
    "            sp3 = []\n",
    "            num_hs = []\n",
    "            for atom in mol.GetAtoms():\n",
    "                type_idx.append(types[atom.GetSymbol()])\n",
    "                atomic_number.append(atom.GetAtomicNum())\n",
    "                aromatic.append(1 if atom.GetIsAromatic() else 0)\n",
    "                hybridization = atom.GetHybridization()\n",
    "                sp.append(1 if hybridization == HybridizationType.SP else 0)\n",
    "                sp2.append(1 if hybridization == HybridizationType.SP2 else 0)\n",
    "                sp3.append(1 if hybridization == HybridizationType.SP3 else 0)\n",
    "\n",
    "            z = torch.tensor(atomic_number, dtype=torch.long)\n",
    "\n",
    "            x = F.one_hot(torch.tensor(type_idx), num_classes=len(types)).float()\n",
    "\n",
    "            y = target[i].unsqueeze(0)\n",
    "            name = mol.GetProp('_Name')\n",
    "\n",
    "            edge_attr, node_attr, edge_dist = self.get_O3_attr(edge_index, pos, self.attr_irreps)\n",
    "\n",
    "            data = Data(x=x, pos=pos, edge_index=edge_index, edge_attr=edge_attr,\n",
    "                        node_attr=node_attr, additional_message_features=edge_dist, y=y, name=name, index=i)\n",
    "            data_list.append(data)\n",
    "\n",
    "        torch.save(self.collate(data_list), self.processed_paths[0])\n",
    "\n",
    "    def get_O3_attr(self, edge_index, pos, attr_irreps):\n",
    "        \"\"\" Creates spherical harmonic edge attributes and node attributes for the SEGNN \"\"\"\n",
    "        rel_pos = pos[edge_index[0]] - pos[edge_index[1]]  # pos_j - pos_i (note in edge_index stores tuples like (j,i))\n",
    "        edge_dist = rel_pos.pow(2).sum(-1, keepdims=True)\n",
    "        edge_attr = spherical_harmonics(attr_irreps, rel_pos, normalize=True,\n",
    "                                        normalization='component')  # Unnormalised for now\n",
    "        node_attr = scatter(edge_attr, edge_index[1], dim=0, reduce=\"mean\")\n",
    "        return edge_attr, node_attr, edge_dist\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length 1000\n",
      "tensor([[1, 2, 3, 4, 0, 2, 3, 4, 0, 1, 3, 4, 0, 1, 2, 4, 0, 1, 2, 3],\n",
      "        [0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4]])\n",
      "mean 53.01295995569229 mad 7.7115207515048985\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    dataset = QM9(\"datasets\", \"alpha\", 2.0, \"train\", 6, feature_type=\"one_hot\")\n",
    "    print(\"length\", len(dataset))\n",
    "    ys = np.array([data.y.item() for data in dataset])\n",
    "    mean, mad = dataset.calc_stats()\n",
    "\n",
    "    for item in dataset:\n",
    "        print(item.edge_index)\n",
    "        break\n",
    "\n",
    "    print(\"mean\", mean, \"mad\", mad)\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    plt.subplot(121)\n",
    "    plt.title(dataset.target)\n",
    "    plt.hist(ys)\n",
    "    plt.subplot(122)\n",
    "    plt.title(dataset.target + \" standardised\")\n",
    "    plt.hist((ys - mean)/mad)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QM9(1000)\n"
     ]
    }
   ],
   "source": [
    "dataset\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[13170, 5], edge_index=[2, 34390], edge_attr=[34390, 49], y=[1000, 19], pos=[13170, 3], node_attr=[13170, 49], additional_message_features=[34390, 1], name=[1000], index=[1000])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data(x=[5, 5], edge_index=[2, 20], edge_attr=[20, 49], y=13.210000038146973, pos=[5, 3], node_attr=[5, 49], additional_message_features=[20, 1], name='gdb_1', index=[1])\n",
      "===============================================================================================================\n",
      "Number of nodes: 5\n",
      "Number of edges: 20\n",
      "Average node degree: 4.00\n",
      "Has isolated nodes: False\n",
      "Has self-loops: False\n",
      "Is undirected: False\n"
     ]
    }
   ],
   "source": [
    "data = dataset[0]  # Get the first graph object.\n",
    "\n",
    "print()\n",
    "print(data)\n",
    "print('===============================================================================================================')\n",
    "# Gather some statistics about the graph.\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "# print(f'Number of training nodes: {data.train_mask.sum()}')\n",
    "# print(f'Training node label rate: {int(data.train_mask.sum()) / data.num_nodes:.3f}')\n",
    "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "print(f'Has self-loops: {data.has_self_loops()}')\n",
    "print(f'Is undirected: {data.is_undirected()}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.distributed as dist\n",
    "from torch.utils.data import Sampler\n",
    "import os\n",
    "import torch\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.utils.data.distributed import DistributedSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataloader(dataset, batch_size, num_workers, world_size = None, rank=None, train=True):\n",
    "    \"\"\" Create (disributed) dataloader \"\"\"\n",
    "\n",
    "    if world_size is not None and world_size > 1:\n",
    "        if train:\n",
    "            sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank)\n",
    "        else:\n",
    "            sampler = DistributedEvalSampler(dataset, num_replicas=world_size, rank=rank)\n",
    "\n",
    "        parallel_batch_size = int(batch_size/world_size)\n",
    "        dataloader = DataLoader(dataset, batch_size=parallel_batch_size, shuffle=(sampler is None),\n",
    "                                sampler=sampler, num_workers=num_workers)\n",
    "    else:\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=train, num_workers=num_workers)\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length 1000\n",
      "tensor([[1, 2, 3, 4, 0, 2, 3, 4, 0, 1, 3, 4, 0, 1, 2, 4, 0, 1, 2, 3],\n",
      "        [0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4]])\n",
      "mean 53.01295995569229 mad 7.7115207515048985\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = QM9(root, target, radius, \"train\", lmax_attr, feature_type)\n",
    "print(\"length\", len(train_dataset))\n",
    "ys = np.array([data.y.item() for data in train_dataset])\n",
    "mean, mad = train_dataset.calc_stats()\n",
    "\n",
    "for item in train_dataset:\n",
    "    print(item.edge_index)\n",
    "    break\n",
    "\n",
    "print(\"mean\", mean, \"mad\", mad)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title(train_dataset.target)\n",
    "plt.hist(ys)\n",
    "plt.subplot(122)\n",
    "plt.title(train_dataset.target + \" standardised\")\n",
    "plt.hist((ys - mean)/mad)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = make_dataloader(train_dataset, batch_size, num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length 178\n",
      "tensor([[1, 0, 2, 1, 3, 2],\n",
      "        [0, 1, 1, 2, 2, 3]])\n",
      "mean 52.574157393380496 mad 7.075225616010086\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXEAAAEICAYAAACpqsStAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAATfklEQVR4nO3dfbBkd13n8feHCZBnk5jJOGEiI8uApuIScAzZjeu6ToKBRBLLiqIlO9Gso7sIwUVxArvu+lC7swVirPJpZwGZVUQiD04qUcw4kEWsEJxAEMKgE5MYYiaZS0xMIlvLJn73jz7Ddi73zu25t093/27er6quPuf06fP7dt/v/dzTp/v0TVUhSWrTM6ZdgCRp+QxxSWqYIS5JDTPEJalhhrgkNcwQl6SGGeJTkuTKJB8b97pa3Z6OfZPkniQXjnF7leT53fRvJvmP49p2t83vSHLfOLd5JMdMaiBJq1OSAjZV1Z3TruVoVdWPT7uGlXJPXNKqlORpsZNqiPcsyfYkf53ksSSfS/I9i6xXSV6X5K4kX0zyliTPmLfOW5M8nOTuJC8fWv7DSfZ3Y9yV5Mf6flzqz6g90607kb5J8vwk/yvJ33fjvLdb/tFulU8neTzJ9yc5NckNSea6cW9IsmFoWzcn+YUkf9aNfVOS04duf3WSv0nyUJI3z6vjvCS3JHkkycEkv5rkWfOej9ckOQAc6Jb9dLfu/Ul+ZN723pXkF7vp07taH0nyd0n+9PBzmeTMJO/vHtPdSV43tI3juu08nORzwLcu9jz2oqq89HgBrgDOZPAH8/uBfwDWA1cCHxtar4CPAKcBXw/8FfBvutuuBP4v8KPAGuDfAvcD6W6/BPgnQIB/CXwJeMm0H7uX8fbMUC9MvG+A9wBv7mo6Fvi2eTU8f2j+a4HvBY4HTgJ+H/iDodtvBv4aeAFwXDe/o7vtbOBx4NuBZwNvA54ALuxu/xbgfAaHgjcC+4HXz6tlT/d8HAdcDDwInAOcAPzucL3Au4Bf7Kb/K/CbwDO7y7/onptnALcBPws8C3gecBfwXd39dgB/2o15FvBZ4L6J9cu0G/bpdgFuBy5b5Jfx4qH5fwfs7aavBO4cuu34bv2vW2SMPwCunvZj9TLenhnqhYn3DfA/gZ3AhgVue0qIL3D7ucDDQ/M3A/9hXs0f6qZ/Fvi9odtOAL58OMQX2PbrgQ/Oq+U7h+bfSfcHopt/wRFC/OeB3fMfC/BS4N55y64Bfqubvmvez2DbJEPcwyk9S/Kvk9zevUR7hMEewemLrP6Foem/YbA3dtgDhyeq6kvd5IndGC9P8vHuJeAjwCuOMIZm3FH2DEymb97IYK/0E0numH9YYl79xyf5790hkUeBjwKnJFmzUF0MXgGc2E2fOfx4quofgIeGtv2C7pDHA922/8sCNQ8/H2fy1c/PYt4C3Anc1B1e2t4tfy5w5uGfR/dcvQlYt4wxxs4Q71GS5wL/A/gJ4Gur6hQGL7WyyF3OGpr+egYvfZca49nA+4G3Auu6Mf7wCGNohi2jZ2ACfVNVD1TVj1bVmcCPAb+e7mN6C3gD8ELgpVV1MoNDIyzxGA47yNDjSXI8g8Mzh/0G8HkGn4Y5mUGYzt/u8FezPmV7DJ6fBVXVY1X1hqp6HvDdwL9PsoVBQN9dVacMXU6qqlcc7Rh9MMT7dQKDhpqDwRtJDPaqFvPT3ZtCZwFXA+8dYYxnMTh2OAc80b1x9bIVVa1pOtqegQn0TZIrht6cfLir8clu/kEGx4kPOwn438AjSU4D/tMI9Rz2PuDSJN/WvWH58zw1p04CHgUeT/KNDI7zH8l1wJVJzu7+ICxaS5JLuzdw043xZHf5BPBokp/p3sRck+ScJIffwLwOuKb7GWwAXnsUj3fFDPEeVdXngF8CbmHQ6N8M/NkR7rKbwRsotwM3Au8YYYzHgNcxaKSHgR8Erl9J3ZqeZfQMTKZvvhW4Ncnj3XpXV9Xd3W3/GdjVHWr4PuBaBm8qfhH4OPChpeoZqusO4DUM3oA82NU2fOLMT3W1PsbgFcsR/2BV1R919XyYwaGSDx9h9U3AnzB4Y/UW4Ner6uaqepLBnvm5wN3d43o78DXd/X6OwSGUu4GbgN8e5bGOy+F3qTVlafiECU2PfSP3xCWpYYa4JDXMwymS1DD3xCWpYRP9gpjTTz+9Nm7cOMkh9TRy2223fbGq1k5jbHtbfTpSb080xDdu3Mi+ffsmOaSeRpJM9Ey5Yfa2+nSk3vZwiiQ1zBCXpIYZ4pLUMENckhpmiEtSwwxxSWqYIS5JDTPEJalhhrgkNWyiZ2xqMjZuv3FZ97tnxyVjrkQaH/t6Ye6JS1LDDHFJapghLkkNM8QlqWGGuCQ1zBCXpIYZ4pLUMENckhpmiEtSwwxxSWqYIS5JDTPEJalhhrgkNcwQl6SGGeKS1DBDXJIaZohLUsMMcUlq2Ej/ni3JPcBjwJPAE1W1OclpwHuBjcA9wPdV1cP9lCn1w95W645mT/xfVdW5VbW5m98O7K2qTcDebl5qkb2tZq3kcMplwK5uehdw+YqrkWaDva1mjBriBdyU5LYk27pl66rqIEB3fcZCd0yyLcm+JPvm5uZWXrE0Xva2mjbSMXHggqq6P8kZwJ4knx91gKraCewE2Lx5cy2jRqlP9raaNtKeeFXd310fAj4InAc8mGQ9QHd9qK8ipb7Y22rdkiGe5IQkJx2eBl4GfBa4HtjarbYV2N1XkVIf7G2tBqMcTlkHfDDJ4fV/t6o+lOTPgeuSXAXcC1zRX5lSL+xtNW/JEK+qu4AXLbD8IWBLH0VJk2BvazXwjE1JapghLkkNM8QlqWGGuCQ1zBCXpIYZ4pLUMENckhpmiEtSwwxxSWrYqN9iqCnYuP3GaZcg9cLeHh/3xCWpYYa4JDXMEJekhhniktQwQ1ySGmaIS1LDDHFJapghLkkNM8QlqWGGuCQ1zBCXpIYZ4pLUMENckhpmiEtSwwxxSWqYIS5JDTPEJalhhrgkNWzkEE+yJsmnktzQzZ+WZE+SA931qf2VKfXDvlbrjmZP/Gpg/9D8dmBvVW0C9nbzUmvsazVtpBBPsgG4BHj70OLLgF3d9C7g8rFWJvXMvtZqMOqe+LXAG4F/HFq2rqoOAnTXZyx0xyTbkuxLsm9ubm4ltUrjdi3L7GuwtzUblgzxJJcCh6rqtuUMUFU7q2pzVW1eu3btcjYhjd1K+xrsbc2GY0ZY5wLglUleARwLnJzkd4AHk6yvqoNJ1gOH+ixUGjP7WqvCknviVXVNVW2oqo3Aq4APV9UPAdcDW7vVtgK7e6tSGjP7WqvFSj4nvgO4KMkB4KJuXmqdfa2mjHI45Suq6mbg5m76IWDL+EuSJsu+Vss8Y1OSGmaIS1LDDHFJapghLkkNM8QlqWGGuCQ1zBCXpIYZ4pLUMENckhpmiEtSwwxxSWqYIS5JDTPEJalhhrgkNcwQl6SGGeKS1DBDXJIaZohLUsMMcUlqmCEuSQ0zxCWpYYa4JDXMEJekhhniktQwQ1ySGmaIS1LDDHFJapghLkkNWzLEkxyb5BNJPp3kjiQ/1y0/LcmeJAe661P7L1caH3tbq8Eoe+L/B/jOqnoRcC5wcZLzge3A3qraBOzt5qWW2Ntq3pIhXgOPd7PP7C4FXAbs6pbvAi7vo0CpL/a2VoORjoknWZPkduAQsKeqbgXWVdVBgO76jEXuuy3JviT75ubmxlS2NB72tlo3UohX1ZNVdS6wATgvyTmjDlBVO6tqc1VtXrt27TLLlPphb6t1R/XplKp6BLgZuBh4MMl6gO760LiLkybF3larRvl0ytokp3TTxwEXAp8Hrge2dqttBXb3VKPUC3tbq8ExI6yzHtiVZA2D0L+uqm5IcgtwXZKrgHuBK3qsU+qDva3mLRniVfUXwIsXWP4QsKWPoqRJsLe1GnjGpiQ1zBCXpIYZ4pLUMENckhpmiEtSw0b5iKFWaOP2G6ddgtQLe3v63BOXpIYZ4pLUMENckhpmiEtSwwxxSWqYIS5JDfMjhvqK5X5c7J4dl4y5Eml8VntfuycuSQ0zxCWpYYa4JDXMEJekhhniktQwQ1ySGmaIS1LDDHFJapghLkkNM8QlqWGGuCQ1zBCXpIYZ4pLUMENckhpmiEtSw5YM8SRnJflIkv1J7khydbf8tCR7khzork/tv1xpfOxtrQaj7Ik/Abyhqr4JOB94TZKzge3A3qraBOzt5qWW2Ntq3pIhXlUHq+qT3fRjwH7gOcBlwK5utV3A5T3VKPXC3tZqcFTHxJNsBF4M3Aqsq6qDMPhlAM5Y5D7bkuxLsm9ubm6F5Ur9sLfVqpFDPMmJwPuB11fVo6Per6p2VtXmqtq8du3a5dQo9creVstGCvEkz2TQ5O+uqg90ix9Msr67fT1wqJ8Spf7Y22rdKJ9OCfAOYH9VvW3opuuBrd30VmD3+MuT+mNvazU4ZoR1LgBeDXwmye3dsjcBO4DrklwF3Atc0UuFUn/sbTVvyRCvqo8BWeTmLeMtR5oce1urgWdsSlLDDHFJapghLkkNM8QlqWGGuCQ1zBCXpIYZ4pLUMENckhpmiEtSwwxxSWqYIS5JDTPEJalhhrgkNcwQl6SGGeKS1DBDXJIaZohLUsMMcUlqmCEuSQ0zxCWpYYa4JDXMEJekhhniktQwQ1ySGmaIS1LDDHFJapghLkkNM8QlqWFLhniSdyY5lOSzQ8tOS7InyYHu+tR+y5TGz97WajDKnvi7gIvnLdsO7K2qTcDebl5qzbuwt9W4JUO8qj4K/N28xZcBu7rpXcDl4y1L6p+9rdVgucfE11XVQYDu+ozFVkyyLcm+JPvm5uaWOZw0Mfa2mtL7G5tVtbOqNlfV5rVr1/Y9nDQx9rZmwXJD/MEk6wG660PjK0maKntbTVluiF8PbO2mtwK7x1OONHX2tpoyykcM3wPcArwwyX1JrgJ2ABclOQBc1M1LTbG3tRocs9QKVfUDi9y0Zcy1SBNlb2s18IxNSWqYIS5JDTPEJalhhrgkNcwQl6SGLfnplNVq4/Ybp13CqrGc5/KeHZf0UIns6/FZ7nM56d52T1ySGmaIS1LDDHFJapghLkkNM8QlqWGGuCQ1zBCXpIYZ4pLUMENckhpmiEtSwwxxSWqYIS5JDTPEJalhhrgkNcwQl6SGGeKS1DBDXJIaZohLUsMMcUlqmCEuSQ0zxCWpYTPz3+79L91PL638J/GVsq+ffibd2yvaE09ycZK/THJnku0r2ZY0S+xttWLZIZ5kDfBrwMuBs4EfSHL2uAqTpsXeVktWsid+HnBnVd1VVV8Gfg+4bDxlSVNlb6sZKzkm/hzgC0Pz9wEvnb9Skm3Atm728SR/uYIxTwe+uIL7T5r1jln+21cmF6r1uWMaZhq9PYpp/nwcu2dDvb3Q2Iv29kpCPAssq69aULUT2LmCcf7/gMm+qto8jm1NgvX2p+daJ97bo5jmz8exZ3fslRxOuQ84a2h+A3D/CrYnzQp7W81YSYj/ObApyTckeRbwKuD68ZQlTZW9rWYs+3BKVT2R5CeAPwbWAO+sqjvGVtnCJvbSdUystz+91Tql3h7FNH8+jj2jY6fqqw71SZIa4Wn3ktQwQ1ySGjaTIZ7krCQfSbI/yR1Jru6Wn5ZkT5ID3fWp0651WJI1ST6V5IZufmbrTXJKkvcl+Xz3PP+zGa/3J7te+GyS9yQ5dpbr7VuSn0pSSU6f4Jhv6frlL5J8MMkpPY83ta8+WCyDJjj+U7LkSGYyxIEngDdU1TcB5wOv6U573g7srapNwN5ufpZcDewfmp/len8F+FBVfSPwIgZ1z2S9SZ4DvA7YXFXnMHiz8VXMaL19S3IWcBFw74SH3gOcU1X/FPgr4Jq+BpqBrz5YLIMmZX6WLGomQ7yqDlbVJ7vpxxg8mOcwOPV5V7faLuDyqRS4gCQbgEuAtw8tnsl6k5wMfDvwDoCq+nJVPcKM1ts5BjguyTHA8Qw+tz3L9fbpl4E3ssAJSH2qqpuq6olu9uMMPj/fl6l+9cERMqh3i2TJomYyxIcl2Qi8GLgVWFdVB2HwJANnTLG0+a5l8Iv1j0PLZrXe5wFzwG91L9nenuQEZrTeqvpb4K0M9jwPAn9fVTcxo/X2Kckrgb+tqk9PuZQfAf6ox+0v9NUHEwnR+eZl0CRcy1dnyaJm5vvEF5LkROD9wOur6tFkobOhpy/JpcChqrotyXdMuZxRHAO8BHhtVd2a5FeY4UMR3bHuy4BvAB4Bfj/JD021qB4l+RPg6xa46c3Am4CXTWPsqtrdrfNmBocb3t1XHYz41Qd9m59BExjvqLNkZkM8yTMZPHnvrqoPdIsfTLK+qg4mWQ8cml6FT3EB8MokrwCOBU5O8jvMbr33AfdV1eE9i/cxCPFZrfdC4O6qmgNI8gHgnzO79a5IVV240PIk38zgD9mnux2aDcAnk5xXVQ/0OfZQDVuBS4Et1e9JJlP/6oNFMqhvC2ZJVS260zKTh1My6NB3APur6m1DN10PbO2mtwK7J13bQqrqmqraUFUbGbzh9uHuSZ/Veh8AvpDkhd2iLcDnmNF6GRxGOT/J8V1vbGFwjHJW6+1FVX2mqs6oqo1dr90HvGRcAb6UJBcDPwO8sqq+1PNwU/3qgyNkUK+OkCWLmtU98QuAVwOfSXJ7t+xNwA7guiRXMfjFvmI65Y1slut9LfDu7hfkLuCHGfxRn7l6u0M+7wM+yeBl/KcYnJJ8IjNY7yr2q8CzgT3dK4GPV9WP9zHQDHz1wYIZVFV/OMEaRuJp95LUsJk8nCJJGo0hLkkNM8QlqWGGuCQ1zBCXpIYZ4pLUMENckhr2/wCsC7O7NyP0LAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "valid_dataset = QM9(root, target, radius, \"valid\", lmax_attr, feature_type)\n",
    "print(\"length\", len(valid_dataset))\n",
    "ys = np.array([data.y.item() for data in valid_dataset])\n",
    "mean, mad = valid_dataset.calc_stats()\n",
    "\n",
    "for item in valid_dataset:\n",
    "    print(item.edge_index)\n",
    "    break\n",
    "\n",
    "print(\"mean\", mean, \"mad\", mad)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title(valid_dataset.target)\n",
    "plt.hist(ys)\n",
    "plt.subplot(122)\n",
    "plt.title(valid_dataset.target + \" standardised\")\n",
    "plt.hist((ys - mean)/mad)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_loader = make_dataloader(valid_dataset, batch_size, num_workers, train=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length 130\n",
      "tensor([[1, 3, 4, 0, 2, 5, 1, 0, 4, 0, 3, 1],\n",
      "        [0, 0, 0, 1, 1, 1, 2, 3, 3, 4, 4, 5]])\n",
      "mean 53.46969212752122 mad 7.9366248969354585\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_dataset = QM9(root, target, radius, \"test\", lmax_attr, feature_type)\n",
    "print(\"length\", len(test_dataset))\n",
    "ys = np.array([data.y.item() for data in test_dataset])\n",
    "mean, mad = test_dataset.calc_stats()\n",
    "\n",
    "for item in test_dataset:\n",
    "    print(item.edge_index)\n",
    "    break\n",
    "\n",
    "print(\"mean\", mean, \"mad\", mad)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title(test_dataset.target)\n",
    "plt.hist(ys)\n",
    "plt.subplot(122)\n",
    "plt.title(test_dataset.target + \" standardised\")\n",
    "plt.hist((ys - mean)/mad)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = make_dataloader(test_dataset, batch_size, num_workers, train=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get train set statistics\n",
    "target_mean, target_mad = train_loader.dataset.calc_stats()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the group representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from e3nn.o3 import Irreps, spherical_harmonics\n",
    "input_irreps = Irreps(\"5x0e\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_irreps = Irreps(\"1x0e\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_attr_irreps = Irreps.spherical_harmonics(3)\n",
    "node_attr_irreps = Irreps.spherical_harmonics(3)\n",
    "additional_message_irreps = Irreps(\"1x0e\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create hidden irreps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from e3nn.o3 import Irreps\n",
    "from e3nn.o3 import Linear, spherical_harmonics, FullyConnectedTensorProduct\n",
    "\n",
    "\n",
    "def BalancedIrreps(lmax, vec_dim, sh_type=True):\n",
    "    \"\"\" Allocates irreps equally along channel budget, resulting\n",
    "        in unequal numbers of irreps in ratios of 2l_i + 1 to 2l_j + 1.\n",
    "    Parameters\n",
    "    ----------\n",
    "    lmax : int\n",
    "        Maximum order of irreps.\n",
    "    vec_dim : int\n",
    "        Dim of feature vector.\n",
    "    sh_type : bool\n",
    "        if true, use spherical harmonics. Else the full set of irreps (with redundance).\n",
    "    Returns\n",
    "    -------\n",
    "    Irreps\n",
    "        Resulting irreps for feature vectors.\n",
    "    \"\"\"\n",
    "    irrep_spec = \"0e\"\n",
    "    for l in range(1, lmax + 1):\n",
    "        if sh_type:\n",
    "            irrep_spec += \" + {0}\".format(l) + ('e' if (l % 2) == 0 else 'o')\n",
    "        else:\n",
    "            irrep_spec += \" + {0}e + {0}o\".format(l)\n",
    "    irrep_spec_split = irrep_spec.split(\" + \")\n",
    "    dims = [int(irrep[0]) * 2 + 1 for irrep in irrep_spec_split]\n",
    "    # Compute ratios\n",
    "    ratios = [1 / dim for dim in dims]\n",
    "    # Determine how many copies per irrep\n",
    "    irrep_copies = [int(vec_dim * r / len(ratios)) for r in ratios]\n",
    "    # Determine the current effective irrep sizes\n",
    "    irrep_dims = [n * dim for (n, dim) in zip(irrep_copies, dims)]\n",
    "    # Add trivial irreps until the desired size is reached\n",
    "    irrep_copies[0] += vec_dim - sum(irrep_dims)\n",
    "\n",
    "    # Convert to string\n",
    "    str_out = ''\n",
    "    for (spec, dim) in zip(irrep_spec_split, irrep_copies):\n",
    "        str_out += str(dim) + 'x' + spec\n",
    "        str_out += ' + '\n",
    "    str_out = str_out[:-3]\n",
    "    # Generate the irrep\n",
    "    return Irreps(str_out)\n",
    "\n",
    "\n",
    "def WeightBalancedIrreps(irreps_in1_scalar, irreps_in2, sh=True, lmax=None):\n",
    "    \"\"\"Determines an irreps_in1 type of order irreps_in2.lmax that when used in a tensor product\n",
    "    irreps_in1 x irreps_in2 -> irreps_in1\n",
    "    would have the same number of weights as for a standard linear layer, e.g. a tensor product\n",
    "    irreps_in1_scalar x \"1x0e\" -> irreps_in1_scalar\n",
    "    Parameters\n",
    "    ----------\n",
    "    irreps_in1_scalar : o3.Irreps\n",
    "        Number of hidden features, represented by zeroth order irreps.\n",
    "    irreps_in2 : o3.Irreps\n",
    "        Irreps related to edge attributes.\n",
    "    sh : bool\n",
    "        if true, yields equal number of every order. Else returns balanced irrep.\n",
    "    lmax : int\n",
    "        Maximum order irreps to be considered.\n",
    "    Returns\n",
    "    -------\n",
    "    o3.Irreps\n",
    "        Irreps for hidden feaure vectors.\n",
    "    \"\"\"\n",
    "\n",
    "    n = 1\n",
    "    if lmax == None:\n",
    "        lmax = irreps_in2.lmax\n",
    "    irreps_in1 = (Irreps.spherical_harmonics(lmax) * n).sort().irreps.simplify() if sh else BalancedIrreps(lmax, n)\n",
    "    weight_numel1 = FullyConnectedTensorProduct(irreps_in1, irreps_in2, irreps_in1).weight_numel\n",
    "    weight_numel_scalar = FullyConnectedTensorProduct(irreps_in1_scalar, Irreps(\"1x0e\"), irreps_in1_scalar).weight_numel\n",
    "    while weight_numel1 < weight_numel_scalar:  # TODO: somewhat suboptimal implementation...\n",
    "        n += 1\n",
    "        irreps_in1 = (Irreps.spherical_harmonics(lmax) * n).sort().irreps.simplify() if sh else BalancedIrreps(lmax, n)\n",
    "        weight_numel1 = FullyConnectedTensorProduct(irreps_in1, irreps_in2, irreps_in1).weight_numel\n",
    "    print('Determined irrep type:', irreps_in1)\n",
    "    return Irreps(irreps_in1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determined irrep type: 9x0e+9x1o+9x2e\n"
     ]
    }
   ],
   "source": [
    "hidden_irreps = WeightBalancedIrreps(\n",
    "            Irreps(\"{}x0e\".format(hidden_features)), node_attr_irreps, sh=True, lmax=lmax_h)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SEGNN Model Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import MessagePassing, global_mean_pool, global_add_pool\n",
    "from e3nn.nn import BatchNorm, Gate\n",
    "from e3nn.o3 import Irreps, Linear, spherical_harmonics, FullyConnectedTensorProduct\n",
    "import numpy as np\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class O3TensorProduct(nn.Module):\n",
    "    \"\"\" A bilinear layer, computing CG tensorproduct and normalising them.\n",
    "    Parameters\n",
    "    ----------\n",
    "    irreps_in1 : o3.Irreps\n",
    "        Input irreps.\n",
    "    irreps_out : o3.Irreps\n",
    "        Output irreps.\n",
    "    irreps_in2 : o3.Irreps\n",
    "        Second input irreps.\n",
    "    tp_rescale : bool\n",
    "        If true, rescales the tensor product.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, irreps_in1, irreps_out, irreps_in2=None, tp_rescale=True) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.irreps_in1 = irreps_in1\n",
    "        self.irreps_out = irreps_out\n",
    "        # Init irreps_in2\n",
    "        if irreps_in2 == None:\n",
    "            self.irreps_in2_provided = False\n",
    "            self.irreps_in2 = Irreps(\"1x0e\")\n",
    "        else:\n",
    "            self.irreps_in2_provided = True\n",
    "            self.irreps_in2 = irreps_in2\n",
    "        self.tp_rescale = tp_rescale\n",
    "\n",
    "        # Build the layers\n",
    "        self.tp = FullyConnectedTensorProduct(\n",
    "            irreps_in1=self.irreps_in1,\n",
    "            irreps_in2=self.irreps_in2,\n",
    "            irreps_out=self.irreps_out, shared_weights=True, normalization='component')\n",
    "\n",
    "        # For each zeroth order output irrep we need a bias\n",
    "        # So first determine the order for each output tensor and their dims\n",
    "        self.irreps_out_orders = [int(irrep_str[-2]) for irrep_str in str(irreps_out).split('+')]\n",
    "        self.irreps_out_dims = [int(irrep_str.split('x')[0]) for irrep_str in str(irreps_out).split('+')]\n",
    "        self.irreps_out_slices = irreps_out.slices()\n",
    "        # Store tuples of slices and corresponding biases in a list\n",
    "        self.biases = []\n",
    "        self.biases_slices = []\n",
    "        self.biases_slice_idx = []\n",
    "        for slice_idx in range(len(self.irreps_out_orders)):\n",
    "            if self.irreps_out_orders[slice_idx] == 0:\n",
    "                out_slice = irreps_out.slices()[slice_idx]\n",
    "                out_bias = torch.zeros(self.irreps_out_dims[slice_idx], dtype=self.tp.weight.dtype)\n",
    "                self.biases += [out_bias]\n",
    "                self.biases_slices += [out_slice]\n",
    "                self.biases_slice_idx += [slice_idx]\n",
    "\n",
    "        # Initialize the correction factors\n",
    "        self.slices_sqrt_k = {}\n",
    "\n",
    "        # Initialize similar to the torch.nn.Linear\n",
    "        self.tensor_product_init()\n",
    "        # Adapt parameters so they can be applied using vector operations.\n",
    "        self.vectorise()\n",
    "\n",
    "    def tensor_product_init(self) -> None:\n",
    "        with torch.no_grad():\n",
    "            # Determine fan_in for each slice, it could be that each output slice is updated via several instructions\n",
    "            slices_fan_in = {}  # fan_in per slice\n",
    "            for weight, instr in zip(self.tp.weight_views(), self.tp.instructions):\n",
    "                slice_idx = instr[2]\n",
    "                mul_1, mul_2, mul_out = weight.shape\n",
    "                fan_in = mul_1 * mul_2\n",
    "                slices_fan_in[slice_idx] = (slices_fan_in[slice_idx] +\n",
    "                                            fan_in if slice_idx in slices_fan_in.keys() else fan_in)\n",
    "            # Do the initialization of the weights in each instruction\n",
    "            for weight, instr in zip(self.tp.weight_views(), self.tp.instructions):\n",
    "                # The tensor product in e3nn already normalizes proportional to 1 / sqrt(fan_in), and the weights are by\n",
    "                # default initialized with unif(-1,1). However, we want to be consistent with torch.nn.Linear and\n",
    "                # initialize the weights with unif(-sqrt(k),sqrt(k)), with k = 1 / fan_in\n",
    "                slice_idx = instr[2]\n",
    "                if self.tp_rescale:\n",
    "                    sqrt_k = 1 / sqrt(slices_fan_in[slice_idx])\n",
    "                else:\n",
    "                    sqrt_k = 1.\n",
    "                weight.data.uniform_(-sqrt_k, sqrt_k)\n",
    "                self.slices_sqrt_k[slice_idx] = (self.irreps_out_slices[slice_idx], sqrt_k)\n",
    "\n",
    "            # Initialize the biases\n",
    "            for (out_slice_idx, out_slice, out_bias) in zip(self.biases_slice_idx, self.biases_slices, self.biases):\n",
    "                sqrt_k = 1 / sqrt(slices_fan_in[out_slice_idx])\n",
    "                out_bias.uniform_(-sqrt_k, sqrt_k)\n",
    "\n",
    "    def vectorise(self):\n",
    "        \"\"\" Adapts the bias parameter and the sqrt_k corrections so they can be applied using vectorised operations \"\"\"\n",
    "\n",
    "        # Vectorise the bias parameters\n",
    "        if len(self.biases) > 0:\n",
    "            with torch.no_grad():\n",
    "                self.biases = torch.cat(self.biases, dim=0)\n",
    "            self.biases = nn.Parameter(self.biases)\n",
    "\n",
    "            # Compute broadcast indices.\n",
    "            bias_idx = torch.LongTensor()\n",
    "            for slice_idx in range(len(self.irreps_out_orders)):\n",
    "                if self.irreps_out_orders[slice_idx] == 0:\n",
    "                    out_slice = self.irreps_out.slices()[slice_idx]\n",
    "                    bias_idx = torch.cat((bias_idx, torch.arange(out_slice.start, out_slice.stop).long()), dim=0)\n",
    "\n",
    "            self.register_buffer(\"bias_idx\", bias_idx, persistent=False)\n",
    "        else:\n",
    "            self.biases = None\n",
    "\n",
    "        # Now onto the sqrt_k correction\n",
    "        sqrt_k_correction = torch.zeros(self.irreps_out.dim)\n",
    "        for instr in self.tp.instructions:\n",
    "            slice_idx = instr[2]\n",
    "            slice, sqrt_k = self.slices_sqrt_k[slice_idx]\n",
    "            sqrt_k_correction[slice] = sqrt_k\n",
    "\n",
    "        # Make sure bias_idx and sqrt_k_correction are on same device as module\n",
    "        self.register_buffer(\"sqrt_k_correction\", sqrt_k_correction, persistent=False)\n",
    "\n",
    "    def forward_tp_rescale_bias(self, data_in1, data_in2=None) -> torch.Tensor:\n",
    "        if data_in2 == None:\n",
    "            data_in2 = torch.ones_like(data_in1[:, 0:1])\n",
    "\n",
    "        data_out = self.tp(data_in1, data_in2)\n",
    "\n",
    "        # Apply corrections\n",
    "        if self.tp_rescale:\n",
    "            data_out /= self.sqrt_k_correction\n",
    "\n",
    "        # Add the biases\n",
    "        if self.biases is not None:\n",
    "            data_out[:, self.bias_idx] += self.biases\n",
    "        return data_out\n",
    "\n",
    "    def forward(self, data_in1, data_in2=None) -> torch.Tensor:\n",
    "        # Apply the tensor product, the rescaling and the bias\n",
    "        data_out = self.forward_tp_rescale_bias(data_in1, data_in2)\n",
    "        return data_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class O3TensorProductSwishGate(O3TensorProduct):\n",
    "    def __init__(self, irreps_in1, irreps_out, irreps_in2=None) -> None:\n",
    "        # For the gate the output of the linear needs to have an extra number of scalar irreps equal to the amount of\n",
    "        # non scalar irreps:\n",
    "        # The first type is assumed to be scalar and passed through the activation\n",
    "        irreps_g_scalars = Irreps(str(irreps_out[0]))\n",
    "        # The remaining types are gated\n",
    "        irreps_g_gate = Irreps(\"{}x0e\".format(irreps_out.num_irreps - irreps_g_scalars.num_irreps))\n",
    "        irreps_g_gated = Irreps(str(irreps_out[1:]))\n",
    "        # So the gate needs the following irrep as input, this is the output irrep of the tensor product\n",
    "        irreps_g = (irreps_g_scalars + irreps_g_gate + irreps_g_gated).simplify()\n",
    "\n",
    "        # Build the layers\n",
    "        super(O3TensorProductSwishGate, self).__init__(irreps_in1, irreps_g, irreps_in2)\n",
    "        if irreps_g_gated.num_irreps > 0:\n",
    "            self.gate = Gate(irreps_g_scalars, [nn.SiLU()], irreps_g_gate, [torch.sigmoid], irreps_g_gated)\n",
    "        else:\n",
    "            self.gate = nn.SiLU()\n",
    "\n",
    "    def forward(self, data_in1, data_in2=None) -> torch.Tensor:\n",
    "        # Apply the tensor product, the rescaling and the bias\n",
    "        data_out = self.forward_tp_rescale_bias(data_in1, data_in2)\n",
    "        # Apply the gate\n",
    "        data_out = self.gate(data_out)\n",
    "        # Return result\n",
    "        return data_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class O3SwishGate(torch.nn.Module):\n",
    "    def __init__(self, irreps_g_scalars, irreps_g_gate, irreps_g_gated) -> None:\n",
    "        super().__init__()\n",
    "        if irreps_g_gated.num_irreps > 0:\n",
    "            self.gate = Gate(irreps_g_scalars, [nn.SiLU()], irreps_g_gate, [torch.sigmoid], irreps_g_gated)\n",
    "        else:\n",
    "            self.gate = nn.SiLU()\n",
    "\n",
    "    def forward(self, data_in) -> torch.Tensor:\n",
    "        data_out = self.gate(data_in)\n",
    "        return data_out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of Instance Normalization is to normalize each feature vector in the input tensor by its norm, which is invariant under orthonormal representations. The normalization is applied separately to each batch element and feature vector. In addition to normalization, Instance Normalization can also apply an affine transformation to each feature vector, consisting of a learned weight and bias parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InstanceNorm(nn.Module):\n",
    "    '''Instance normalization for orthonormal representations\n",
    "    It normalizes by the norm of the representations.\n",
    "    Note that the norm is invariant only for orthonormal representations.\n",
    "    Irreducible representations `wigner_D` are orthonormal.\n",
    "    Parameters\n",
    "    ----------\n",
    "    irreps : `Irreps`\n",
    "        representation\n",
    "    eps : float\n",
    "        avoid division by zero when we normalize by the variance\n",
    "    affine : bool\n",
    "        do we have weight and bias parameters\n",
    "    reduce : {'mean', 'max'}\n",
    "        method used to reduce\n",
    "    '''\n",
    "\n",
    "    def __init__(self, irreps, eps=1e-5, affine=True, reduce='mean', normalization='component'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.irreps = Irreps(irreps)\n",
    "        self.eps = eps\n",
    "        self.affine = affine\n",
    "\n",
    "        num_scalar = sum(mul for mul, ir in self.irreps if ir.l == 0)\n",
    "        num_features = self.irreps.num_irreps\n",
    "\n",
    "        if affine:\n",
    "            self.weight = nn.Parameter(torch.ones(num_features))\n",
    "            self.bias = nn.Parameter(torch.zeros(num_scalar))\n",
    "        else:\n",
    "            self.register_parameter('weight', None)\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        assert isinstance(reduce, str), \"reduce should be passed as a string value\"\n",
    "        assert reduce in ['mean', 'max'], \"reduce needs to be 'mean' or 'max'\"\n",
    "        self.reduce = reduce\n",
    "\n",
    "        assert normalization in ['norm', 'component'], \"normalization needs to be 'norm' or 'component'\"\n",
    "        self.normalization = normalization\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.__class__.__name__} ({self.irreps}, eps={self.eps})\"\n",
    "\n",
    "    def forward(self, input, batch):\n",
    "        '''evaluate\n",
    "        Parameters\n",
    "        ----------\n",
    "        input : `torch.Tensor`\n",
    "            tensor of shape ``(batch, ..., irreps.dim)``\n",
    "        Returns\n",
    "        -------\n",
    "        `torch.Tensor`\n",
    "            tensor of shape ``(batch, ..., irreps.dim)``\n",
    "        '''\n",
    "        # batch, *size, dim = input.shape  # TODO: deal with batch\n",
    "        # input = input.reshape(batch, -1, dim)  # [batch, sample, stacked features]\n",
    "        # input has shape [batch * nodes, dim], but with variable nr of nodes.\n",
    "        # the input batch slices this into separate graphs\n",
    "        dim = input.shape[-1]\n",
    "\n",
    "        fields = []\n",
    "        ix = 0\n",
    "        iw = 0\n",
    "        ib = 0\n",
    "\n",
    "        for mul, ir in self.irreps:  # mul is the multiplicity (number of copies) of some irrep type (ir)\n",
    "            d = ir.dim\n",
    "            field = input[:, ix: ix + mul * d]  # [batch * sample, mul * repr]\n",
    "            ix += mul * d\n",
    "\n",
    "            # [batch * sample, mul, repr]\n",
    "            field = field.reshape(-1, mul, d)\n",
    "\n",
    "            # For scalars first compute and subtract the mean\n",
    "            if ir.l == 0:\n",
    "                # Compute the mean\n",
    "                field_mean = global_mean_pool(field, batch).reshape(-1, mul, 1)  # [batch, mul, 1]]\n",
    "                # Subtract the mean\n",
    "                field = field - field_mean[batch]\n",
    "\n",
    "            # Then compute the rescaling factor (norm of each feature vector)\n",
    "            # Rescaling of the norms themselves based on the option \"normalization\"\n",
    "            if self.normalization == 'norm':\n",
    "                field_norm = field.pow(2).sum(-1)  # [batch * sample, mul]\n",
    "            elif self.normalization == 'component':\n",
    "                field_norm = field.pow(2).mean(-1)  # [batch * sample, mul]\n",
    "            else:\n",
    "                raise ValueError(\"Invalid normalization option {}\".format(self.normalization))\n",
    "            # Reduction method\n",
    "            if self.reduce == 'mean':\n",
    "                field_norm = global_mean_pool(field_norm, batch)  # [batch, mul]\n",
    "            elif self.reduce == 'max':\n",
    "                field_norm = global_max_pool(field_norm, batch)  # [batch, mul]\n",
    "            else:\n",
    "                raise ValueError(\"Invalid reduce option {}\".format(self.reduce))\n",
    "\n",
    "            # Then apply the rescaling (divide by the sqrt of the squared_norm, i.e., divide by the norm\n",
    "            field_norm = (field_norm + self.eps).pow(-0.5)  # [batch, mul]\n",
    "\n",
    "            if self.affine:\n",
    "                weight = self.weight[None, iw: iw + mul]  # [batch, mul]\n",
    "                iw += mul\n",
    "                field_norm = field_norm * weight  # [batch, mul]\n",
    "\n",
    "            field = field * field_norm[batch].reshape(-1, mul, 1)  # [batch * sample, mul, repr]\n",
    "\n",
    "            if self.affine and d == 1:  # scalars\n",
    "                bias = self.bias[ib: ib + mul]  # [batch, mul]\n",
    "                ib += mul\n",
    "                field += bias.reshape(mul, 1)  # [batch * sample, mul, repr]\n",
    "\n",
    "            # Save the result, to be stacked later with the rest\n",
    "            fields.append(field.reshape(-1, mul * d))  # [batch * sample, mul * repr]\n",
    "\n",
    "        if ix != dim:\n",
    "            fmt = \"`ix` should have reached input.size(-1) ({}), but it ended at {}\"\n",
    "            msg = fmt.format(dim, ix)\n",
    "            raise AssertionError(msg)\n",
    "\n",
    "        output = torch.cat(fields, dim=-1)  # [batch * sample, stacked features]\n",
    "        return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SEGNN Implementation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Message Passing layer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The constructor initializes several layers for message passing and node updates. Specifically, it initializes two O3TensorProductSwishGate layers for computing messages, and two O3TensorProduct layers for updating node features. It also sets up normalisation, either batch or instance norm.\n",
    "\n",
    "The forward method takes node features, edge indices, edge features, node attributes, batch indices, and additional message features as input, and propagates messages along edges using the propagate method of MessagePassing. It then applies normalisation to the resulting node features.\n",
    "\n",
    "The message method computes messages using the message_layer_1 and message_layer_2 layers, and applies normalisation to the resulting messages. It takes input node features of sender and receiver nodes, edge attributes, and additional message features as input.\n",
    "\n",
    "The update method updates node features using the update_layer_1 and update_layer_2 layers, and adds a residual connection to the original node features. It takes messages, node features, and node attributes as input, and returns updated node features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SEGNNLayer(MessagePassing):\n",
    "    \"\"\"E(3) equivariant message passing layer.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_irreps,\n",
    "        hidden_irreps,\n",
    "        output_irreps,\n",
    "        edge_attr_irreps,\n",
    "        node_attr_irreps,\n",
    "        norm=None,\n",
    "        additional_message_irreps=None,\n",
    "    ):\n",
    "        super().__init__(node_dim=-2, aggr=\"add\")\n",
    "        self.hidden_irreps = hidden_irreps\n",
    "\n",
    "        message_input_irreps = (2 * input_irreps + additional_message_irreps).simplify()\n",
    "        update_input_irreps = (input_irreps + hidden_irreps).simplify()\n",
    "\n",
    "        self.message_layer_1 = O3TensorProductSwishGate(\n",
    "            message_input_irreps, hidden_irreps, edge_attr_irreps\n",
    "        )\n",
    "        self.message_layer_2 = O3TensorProductSwishGate(\n",
    "            hidden_irreps, hidden_irreps, edge_attr_irreps\n",
    "        )\n",
    "        self.update_layer_1 = O3TensorProductSwishGate(\n",
    "            update_input_irreps, hidden_irreps, node_attr_irreps\n",
    "        )\n",
    "        self.update_layer_2 = O3TensorProduct(\n",
    "            hidden_irreps, hidden_irreps, node_attr_irreps\n",
    "        )\n",
    "\n",
    "        self.setup_normalisation(norm)\n",
    "\n",
    "    def setup_normalisation(self, norm):\n",
    "        \"\"\"Set up normalisation, either batch or instance norm\"\"\"\n",
    "        self.norm = norm\n",
    "        self.feature_norm = None\n",
    "        self.message_norm = None\n",
    "\n",
    "        if norm == \"batch\":\n",
    "            self.feature_norm = BatchNorm(self.hidden_irreps)\n",
    "            self.message_norm = BatchNorm(self.hidden_irreps)\n",
    "        elif norm == \"instance\":\n",
    "            self.feature_norm = InstanceNorm(self.hidden_irreps)\n",
    "\n",
    "        self.feature_norm = InstanceNorm(self.hidden_irreps)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "        edge_index,\n",
    "        edge_attr,\n",
    "        node_attr,\n",
    "        batch,\n",
    "        additional_message_features=None,\n",
    "    ):\n",
    "        \"\"\"Propagate messages along edges\"\"\"\n",
    "        x = self.propagate(\n",
    "            edge_index,\n",
    "            x=x,\n",
    "            node_attr=node_attr,\n",
    "            edge_attr=edge_attr,\n",
    "            additional_message_features=additional_message_features,\n",
    "        )\n",
    "        # Normalise features\n",
    "        if self.feature_norm:\n",
    "            if self.norm == \"batch\":\n",
    "                x = self.feature_norm(x)\n",
    "            elif self.norm == \"instance\":\n",
    "                x = self.feature_norm(x, batch)\n",
    "            x = self.feature_norm(x, batch)\n",
    "        return x\n",
    "\n",
    "    def message(self, x_i, x_j, edge_attr, additional_message_features):\n",
    "        \"\"\"Create messages\"\"\"\n",
    "        if additional_message_features is None:\n",
    "            input = torch.cat((x_i, x_j), dim=-1)\n",
    "        else:\n",
    "            input = torch.cat((x_i, x_j, additional_message_features), dim=-1)\n",
    "\n",
    "        message = self.message_layer_1(input, edge_attr)\n",
    "        message = self.message_layer_2(message, edge_attr)\n",
    "\n",
    "        if self.message_norm:\n",
    "            message = self.message_norm(message)\n",
    "        return message\n",
    "\n",
    "    def update(self, message, x, node_attr):\n",
    "        \"\"\"Update note features\"\"\"\n",
    "        input = torch.cat((x, message), dim=-1)\n",
    "        update = self.update_layer_1(input, node_attr)\n",
    "        update = self.update_layer_2(update, node_attr)\n",
    "        x += update  # Residual connection\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steerable modules"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `__init__` method initializes the neural network's architecture, which consists of an embedding layer, several message passing layers, and output layers. The `forward` method performs the forward pass of the neural network on a graph object.\n",
    "\n",
    "During the forward pass, the graph object's features are first embedded using the `embedding_layer`. The resulting node features are then passed through the layers, which perform message passing between nodes. The output from the message passing layers is then processed by the `pre_pool1` and `pre_pool2` layers to prepare it for pooling.\n",
    "\n",
    "The pooling method is determined by the pool argument passed to the SEGNN constructor. Currently, the module supports average and sum pooling. If the output task is graph-level, the pooled output is passed through the `post_pool1` and `post_pool2` layers to produce the final output. If the output task is node-level, the pooled output is passed through the `pre_pool2` layer to produce the final output.\n",
    "\n",
    "The `catch_isolated_nodes` method is used to handle isolated nodes in the input graph, which should still receive node attributes. If the input graph contains isolated nodes, new attributes are added to the graph object to ensure that every node has an attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import TransformerConv, global_mean_pool\n",
    "\n",
    "class SEGNN(nn.Module):\n",
    "    \"\"\"Steerable E(3) equivariant message passing network\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_irreps,\n",
    "        hidden_irreps,\n",
    "        output_irreps,\n",
    "        edge_attr_irreps,\n",
    "        node_attr_irreps,\n",
    "        num_layers,\n",
    "        norm=None,\n",
    "        pool=\"avg\",\n",
    "        task=\"graph\",\n",
    "        additional_message_irreps=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.task = task\n",
    "        self.embedding_layer = O3TensorProduct(\n",
    "            input_irreps, hidden_irreps, node_attr_irreps\n",
    "        )\n",
    "\n",
    "        # Message passing layers.\n",
    "        segnn_layers = []\n",
    "        for i in range(num_layers):\n",
    "            segnn_layers.append(\n",
    "                SEGNNLayer(\n",
    "                    hidden_irreps,\n",
    "                    hidden_irreps,\n",
    "                    hidden_irreps,\n",
    "                    edge_attr_irreps,\n",
    "                    node_attr_irreps,\n",
    "                    norm=norm,\n",
    "                    additional_message_irreps=additional_message_irreps,\n",
    "                )\n",
    "            )\n",
    "        self.segnn_layers = nn.ModuleList(segnn_layers)\n",
    "\n",
    "        # TransformerConv layers.\n",
    "        transformer_layers = []\n",
    "        for i in range(num_layers):\n",
    "            transformer_layers.append(\n",
    "                TransformerConv(\n",
    "                    hidden_irreps.num_irreps,\n",
    "                    hidden_irreps.num_irreps,\n",
    "                    8,\n",
    "                    edge_attr_irreps.dim(),\n",
    "                    node_attr_irreps.dim(),\n",
    "                )\n",
    "            )\n",
    "        self.transformer_layers = nn.ModuleList(transformer_layers)\n",
    "\n",
    "        # Prepare for output irreps, since the attrs will disappear after pooling\n",
    "        if task == \"graph\":\n",
    "            pooled_irreps = (\n",
    "                (output_irreps * hidden_irreps.num_irreps).simplify().sort().irreps\n",
    "            )\n",
    "            self.pre_pool1 = O3TensorProductSwishGate(\n",
    "                hidden_irreps, hidden_irreps, node_attr_irreps\n",
    "            )\n",
    "            self.pre_pool2 = O3TensorProduct(\n",
    "                hidden_irreps, pooled_irreps, node_attr_irreps\n",
    "            )\n",
    "            self.post_pool1 = O3TensorProductSwishGate(pooled_irreps, pooled_irreps)\n",
    "            self.post_pool2 = O3TensorProduct(pooled_irreps, output_irreps)\n",
    "            self.init_pooler(pool)\n",
    "        elif task == \"node\":\n",
    "            self.pre_pool1 = O3TensorProductSwishGate(\n",
    "                hidden_irreps, hidden_irreps, node_attr_irreps\n",
    "            )\n",
    "            self.pre_pool2 = O3TensorProduct(\n",
    "                hidden_irreps, output_irreps, node_attr_irreps\n",
    "            )\n",
    "\n",
    "    def init_pooler(self, pool):\n",
    "        \"\"\"Initialise pooling mechanism\"\"\"\n",
    "        if pool == \"avg\":\n",
    "            self.pooler = global_mean_pool\n",
    "        elif pool == \"sum\":\n",
    "            self.pooler = global_add_pool\n",
    "\n",
    "    def catch_isolated_nodes(self, graph):\n",
    "        \"\"\"Isolated nodes should also obtain attributes\"\"\"\n",
    "        if (\n",
    "            graph.contains_isolated_nodes()\n",
    "            and graph.edge_index.max().item() + 1 != graph.num_nodes\n",
    "        ):\n",
    "            nr_add_attr = graph.num_nodes - (graph.edge_index.max().item() + 1)\n",
    "            add_attr = graph.node_attr.new_tensor(\n",
    "                np.zeros((nr_add_attr, node_attr.shape[-1]))\n",
    "            )\n",
    "            graph.node_attr = torch.cat((graph.node_attr, add_attr), -2)\n",
    "        # Trivial irrep value should always be 1 (is automatically so for connected nodes, but isolated nodes are now 0)\n",
    "        graph.node_attr[:, 0] = 1.0\n",
    "\n",
    "    def forward(self, graph):\n",
    "        \"\"\"SEGNN forward pass\"\"\"\n",
    "        x, pos, edge_index, edge_attr, node_attr, batch = (\n",
    "            graph.x,\n",
    "            graph.pos,\n",
    "            graph.edge_index,\n",
    "            graph.edge_attr,\n",
    "            graph.node_attr,\n",
    "            graph.batch,\n",
    "        )\n",
    "        try:\n",
    "            additional_message_features = graph.additional_message_features\n",
    "        except AttributeError:\n",
    "            additional_message_features = None\n",
    "\n",
    "        self.catch_isolated_nodes(graph)\n",
    "\n",
    "        # Embed\n",
    "        x = self.embedding_layer(x, node_attr)\n",
    "\n",
    "        # Pass messages using SEGNN layers\n",
    "        segnn_output = x\n",
    "        for layer in self.segnn_layers:\n",
    "            segnn_output = layer(\n",
    "                segnn_output, edge_index, edge_attr, node_attr, batch, additional_message_features\n",
    "            )\n",
    "\n",
    "        # Pass messages using TransformerConv layers\n",
    "        transformer_output = x\n",
    "        for layer in self.transformer_layers:\n",
    "            transformer_output = layer(transformer_output, edge_index)\n",
    "\n",
    "        # Combine SEGNN and TransformerConv outputs\n",
    "        x = segnn_output + transformer_output\n",
    "\n",
    "        # Pre pool\n",
    "        x = self.pre_pool1(x, node_attr)\n",
    "        x = self.pre_pool2(x, node_attr)\n",
    "\n",
    "        if self.task == \"graph\":\n",
    "            # Pool over nodes\n",
    "            x = self.pooler(x, batch)\n",
    "\n",
    "            # Predict\n",
    "            x = self.post_pool1(x)\n",
    "            x = self.post_pool2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[128], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m SEGNN(input_irreps,\n\u001b[1;32m      2\u001b[0m               hidden_irreps,\n\u001b[1;32m      3\u001b[0m               output_irreps,\n\u001b[1;32m      4\u001b[0m               edge_attr_irreps,\n\u001b[1;32m      5\u001b[0m               node_attr_irreps,\n\u001b[1;32m      6\u001b[0m               num_layers\u001b[39m=\u001b[39;49mlayers,\n\u001b[1;32m      7\u001b[0m               norm\u001b[39m=\u001b[39;49mnorm,\n\u001b[1;32m      8\u001b[0m               pool\u001b[39m=\u001b[39;49mpool,\n\u001b[1;32m      9\u001b[0m               task\u001b[39m=\u001b[39;49mtask,\n\u001b[1;32m     10\u001b[0m               additional_message_irreps\u001b[39m=\u001b[39;49madditional_message_irreps)\n",
      "Cell \u001b[0;32mIn[127], line 49\u001b[0m, in \u001b[0;36mSEGNN.__init__\u001b[0;34m(self, input_irreps, hidden_irreps, output_irreps, edge_attr_irreps, node_attr_irreps, num_layers, norm, pool, task, additional_message_irreps)\u001b[0m\n\u001b[1;32m     42\u001b[0m transformer_layers \u001b[39m=\u001b[39m []\n\u001b[1;32m     43\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_layers):\n\u001b[1;32m     44\u001b[0m     transformer_layers\u001b[39m.\u001b[39mappend(\n\u001b[1;32m     45\u001b[0m         TransformerConv(\n\u001b[1;32m     46\u001b[0m             hidden_irreps\u001b[39m.\u001b[39mnum_irreps,\n\u001b[1;32m     47\u001b[0m             hidden_irreps\u001b[39m.\u001b[39mnum_irreps,\n\u001b[1;32m     48\u001b[0m             \u001b[39m8\u001b[39m,\n\u001b[0;32m---> 49\u001b[0m             edge_attr_irreps\u001b[39m.\u001b[39;49mdim(),\n\u001b[1;32m     50\u001b[0m             node_attr_irreps\u001b[39m.\u001b[39mdim(),\n\u001b[1;32m     51\u001b[0m         )\n\u001b[1;32m     52\u001b[0m     )\n\u001b[1;32m     53\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer_layers \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mModuleList(transformer_layers)\n\u001b[1;32m     55\u001b[0m \u001b[39m# Prepare for output irreps, since the attrs will disappear after pooling\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not callable"
     ]
    }
   ],
   "source": [
    "model = SEGNN(input_irreps,\n",
    "              hidden_irreps,\n",
    "              output_irreps,\n",
    "              edge_attr_irreps,\n",
    "              node_attr_irreps,\n",
    "              num_layers=layers,\n",
    "              norm=norm,\n",
    "              pool=pool,\n",
    "              task=task,\n",
    "              additional_message_irreps=additional_message_irreps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.QM9'>\n"
     ]
    }
   ],
   "source": [
    "print(type(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = \"_\".join([\"segnn\", str(dataset), target, str(np.random.randint(1e4, 1e5))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'segnn_QM9(1000)_alpha_61596'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEGNN(\n",
      "  (embedding_layer): O3TensorProduct(\n",
      "    (tp): FullyConnectedTensorProduct(5x0e x 1x0e+1x1o+1x2e+1x3o -> 9x0e+9x1o+9x2e | 135 paths | 135 weights)\n",
      "  )\n",
      "  (segnn_layers): ModuleList(\n",
      "    (0): SEGNNLayer()\n",
      "    (1): SEGNNLayer()\n",
      "    (2): SEGNNLayer()\n",
      "  )\n",
      "  (transformer_layers): ModuleList(\n",
      "    (0): TransformerConv(27, 27, heads=8)\n",
      "    (1): TransformerConv(27, 27, heads=8)\n",
      "    (2): TransformerConv(27, 27, heads=8)\n",
      "  )\n",
      "  (pre_pool1): O3TensorProductSwishGate(\n",
      "    (tp): FullyConnectedTensorProduct(9x0e+9x1o+9x2e x 1x0e+1x1o+1x2e+1x3o -> 27x0e+9x1o+9x2e | 1539 paths | 1539 weights)\n",
      "    (gate): Gate (27x0e+9x1o+9x2e -> 9x0e+9x1o+9x2e)\n",
      "  )\n",
      "  (pre_pool2): O3TensorProduct(\n",
      "    (tp): FullyConnectedTensorProduct(9x0e+9x1o+9x2e x 1x0e+1x1o+1x2e+1x3o -> 27x0e | 729 paths | 729 weights)\n",
      "  )\n",
      "  (post_pool1): O3TensorProductSwishGate(\n",
      "    (tp): FullyConnectedTensorProduct(27x0e x 1x0e -> 27x0e | 729 paths | 729 weights)\n",
      "    (gate): SiLU()\n",
      "  )\n",
      "  (post_pool2): O3TensorProduct(\n",
      "    (tp): FullyConnectedTensorProduct(27x0e x 1x0e -> 1x0e | 27 paths | 27 weights)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 104,527 parameters.\n"
     ]
    }
   ],
   "source": [
    "print(\"The model has {:,} parameters.\".format(sum(p.numel() for p in model.parameters())))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "from torch.optim.lr_scheduler import MultiStepLR"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's define evaluation function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In here, the `N` tensor will be used to keep track of the total number of samples in the dataloader, and `score` tensor will be used to keep track of the sum of the losses computed for each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion, device, loc=0, scale=1):\n",
    "    \"\"\" Evaluate a model on the dataloader, with distributed communication (if necessary) \"\"\"\n",
    "    model.eval()\n",
    "    N = torch.zeros(1).to(device)\n",
    "    score = torch.zeros(1).to(device)\n",
    "\n",
    "    with torch.no_grad():  #ensure that the computation graph is not built, which saves memory and speeds up the process. \n",
    "        for graph in dataloader:\n",
    "            graph = graph.to(device)\n",
    "            out = model(graph).squeeze()\n",
    "\n",
    "            n = graph.y.size(0)\n",
    "            N += n\n",
    "            score += n*criterion(out*scale + loc, graph.y)\n",
    "\n",
    "    model.train()\n",
    "    # if world_size > 1:\n",
    "    #     dist.all_reduce(score)\n",
    "    #     dist.all_reduce(N)\n",
    "\n",
    "    return (score/N).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def save_model(model, dir, id, gpu=\"\"):\n",
    "    \"\"\" Save a model \"\"\"\n",
    "    os.makedirs(dir, exist_ok=True)\n",
    "    if gpu != \"\":\n",
    "        gpu = \"_\" + str(gpu)\n",
    "\n",
    "    torch.save(model.state_dict(), os.path.join(dir, id + gpu + \".pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model, dir, id, gpu=\"\"):\n",
    "    \"\"\" Load a state dict into a model \"\"\"\n",
    "    if gpu != \"\":\n",
    "        gpu = \"_\" + str(gpu)\n",
    "    state_dict = torch.load(os.path.join(dir, id + gpu + \".pt\"))\n",
    "    model.load_state_dict(state_dict)\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define training function.\n",
    "\n",
    "The function first creates dataloaders for the training, validation, and test sets using the make_dataloader() function. It also calculates the mean and MAD (median absolute deviation) of the target variable in the training set.\n",
    "\n",
    "The optimizer and learning rate scheduler are set up next, followed by the loss function (in this case, L1 loss). The function then initializes the logging and wandb parameters (if logging is enabled) and starts the training loop.\n",
    "\n",
    "For each epoch, the function loops through the training set and trains the model using the optimizer and loss function. The function also logs training statistics and evaluates the model on the validation set after each epoch. The learning rate is adjusted using the scheduler.\n",
    "\n",
    "Finally, the function evaluates the model on the test set and logs the test MAE. If logging is enabled, the function saves the trained model to disk and finishes the logging. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_variable=100\n",
    "lr = 5e-4 #learning rate\n",
    "weight_decay=1e-8\n",
    "def train(gpu, model):\n",
    "    \n",
    "    device = 'cuda:' + str(gpu)\n",
    "    \n",
    "    model = model.to(device)\n",
    "\n",
    "    # Create datasets and dataloaders\n",
    "    # train_loader = utils.make_dataloader(QM9(args.root, args.target, args.radius, \"train\", args.lmax_attr,\n",
    "    #                                          feature_type=args.feature_type), args.batch_size, args.num_workers, args.gpus, gpu)\n",
    "    # valid_loader = utils.make_dataloader(QM9(args.root, args.target, args.radius, \"valid\", args.lmax_attr,\n",
    "    #                                          feature_type=args.feature_type), args.batch_size, args.num_workers, args.gpus, gpu, train=False)\n",
    "    # test_loader = utils.make_dataloader(QM9(args.root, args.target, args.radius, \"test\", args.lmax_attr,\n",
    "    #                                         feature_type=args.feature_type), args.batch_size, args.num_workers, args.gpus, gpu, train=False)\n",
    "\n",
    "    # Get train set statistics\n",
    "    target_mean, target_mad = train_loader.dataset.calc_stats()\n",
    "\n",
    "    # Set up optimizer and loss function\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = MultiStepLR(optimizer, milestones=[int(0.8*(epochs)), int(0.9*(epochs))], verbose=True)\n",
    "    criterion = nn.L1Loss()\n",
    "\n",
    "    # Logging parameters\n",
    "    # target = args.target\n",
    "    best_valid_MAE = 1e30\n",
    "    i = 0\n",
    "    N_samples = 0\n",
    "    loss_sum = 0\n",
    "    train_MAE_sum = 0\n",
    "\n",
    "    # # Init wandb\n",
    "    # if log and gpu == 0:\n",
    "    #     wandb.init(project=\"SEGNN \" + str(dataset) + \" \" + target, name=ID, entity=\"segnn\")\n",
    "\n",
    "    # Let's start!\n",
    "    # if gpu == 0:\n",
    "    print(\"Training:\", ID)\n",
    "    for epoch in range(epochs):\n",
    "        # # Set epoch so shuffling works right in distributed mode.\n",
    "        # if args.gpus > 1:\n",
    "        #     train_loader.sampler.set_epoch(epoch)\n",
    "        # Training loop\n",
    "\n",
    "        for step, graph in enumerate(train_loader):\n",
    "            # Forward & Backward.\n",
    "            graph = graph.to(device)\n",
    "            out = model(graph).squeeze()\n",
    "            loss = criterion(out, (graph.y - target_mean)/target_mad)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Logging\n",
    "            i += 1\n",
    "            N_samples += graph.y.size(0)\n",
    "            loss_sum += loss\n",
    "            train_MAE_sum += criterion(out.detach()*target_mad + target_mean, graph.y)*graph.y.size(0)\n",
    "\n",
    "            # Report\n",
    "            if i % print_variable == 0:\n",
    "                print(\"epoch:%2d  step:%4d  loss: %0.4f  train MAE:%0.4f\" %\n",
    "                      (epoch, step, loss_sum/i, train_MAE_sum/N_samples))\n",
    "\n",
    "                # if log and gpu == 0:\n",
    "                #     wandb.log({\"loss\": loss_sum/i, target + \" train MAE\": train_MAE_sum /\n",
    "                #                N_samples})\n",
    "\n",
    "                i = 0\n",
    "                N_samples = 0\n",
    "                loss_sum = 0\n",
    "                train_MAE_sum = 0\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        valid_MAE = evaluate(model, valid_loader, criterion, device, target_mean, target_mad)\n",
    "        # Save best validation model\n",
    "        if valid_MAE < best_valid_MAE:\n",
    "            best_valid_MAE = valid_MAE\n",
    "            save_model(model, save_dir, ID, device)\n",
    "        # if gpu == 0:\n",
    "        print(\"VALIDATION: epoch:%2d  step:%4d  %s-MAE:%0.4f\" %\n",
    "                (epoch, step, target, valid_MAE))\n",
    "            # if log:\n",
    "            #     wandb.log({target + \" val MAE\": valid_MAE})\n",
    "\n",
    "        # Adapt learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "    # Final evaluation on test set\n",
    "    model = load_model(model, save_dir, ID, device)\n",
    "    test_MAE = evaluate(model, test_loader, criterion, device, target_mean, target_mad, )\n",
    "    # if gpu == 0:\n",
    "    print(\"TEST: epoch:%2d  step:%4d  %s-MAE:%0.4f\" %\n",
    "            (epoch, step, target, test_MAE))\n",
    "    #     if log:\n",
    "    #         wandb.log({target + \" test MAE\": test_MAE})\n",
    "    #         wandb.save(os.path.join(save_dir, ID + \"_\" + device + \".pt\"))\n",
    "\n",
    "    # if log and gpu == 0:\n",
    "    #     wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training on a single gpu...\n",
      "Adjusting learning rate of group 0 to 5.0000e-04.\n",
      "Training: segnn_QM9(1000)_alpha_61596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/intern/anaconda3/envs/envname/lib/python3.8/site-packages/torch_geometric/deprecation.py:13: UserWarning: 'contains_isolated_nodes' is deprecated, use 'has_isolated_nodes' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1690x81 and 27x216)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[126], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mStarting training on a single gpu...\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m mode \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mgpu\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m train(\u001b[39m0\u001b[39;49m, model)\n",
      "Cell \u001b[0;32mIn[125], line 50\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(gpu, model)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mfor\u001b[39;00m step, graph \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[1;32m     48\u001b[0m     \u001b[39m# Forward & Backward.\u001b[39;00m\n\u001b[1;32m     49\u001b[0m     graph \u001b[39m=\u001b[39m graph\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> 50\u001b[0m     out \u001b[39m=\u001b[39m model(graph)\u001b[39m.\u001b[39msqueeze()\n\u001b[1;32m     51\u001b[0m     loss \u001b[39m=\u001b[39m criterion(out, (graph\u001b[39m.\u001b[39my \u001b[39m-\u001b[39m target_mean)\u001b[39m/\u001b[39mtarget_mad)\n\u001b[1;32m     52\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/anaconda3/envs/envname/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[112], line 128\u001b[0m, in \u001b[0;36mSEGNN.forward\u001b[0;34m(self, graph)\u001b[0m\n\u001b[1;32m    126\u001b[0m transformer_output \u001b[39m=\u001b[39m x\n\u001b[1;32m    127\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer_layers:\n\u001b[0;32m--> 128\u001b[0m     transformer_output \u001b[39m=\u001b[39m layer(\n\u001b[1;32m    129\u001b[0m         transformer_output, edge_index, edge_attr, node_attr\n\u001b[1;32m    130\u001b[0m     )\n\u001b[1;32m    132\u001b[0m \u001b[39m# Combine SEGNN and TransformerConv outputs\u001b[39;00m\n\u001b[1;32m    133\u001b[0m x \u001b[39m=\u001b[39m segnn_output \u001b[39m+\u001b[39m transformer_output\n",
      "File \u001b[0;32m~/anaconda3/envs/envname/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/envname/lib/python3.8/site-packages/torch_geometric/nn/conv/transformer_conv.py:171\u001b[0m, in \u001b[0;36mTransformerConv.forward\u001b[0;34m(self, x, edge_index, edge_attr, return_attention_weights)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(x, Tensor):\n\u001b[1;32m    169\u001b[0m     x: PairTensor \u001b[39m=\u001b[39m (x, x)\n\u001b[0;32m--> 171\u001b[0m query \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlin_query(x[\u001b[39m1\u001b[39;49m])\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, H, C)\n\u001b[1;32m    172\u001b[0m key \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlin_key(x[\u001b[39m0\u001b[39m])\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, H, C)\n\u001b[1;32m    173\u001b[0m value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlin_value(x[\u001b[39m0\u001b[39m])\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, H, C)\n",
      "File \u001b[0;32m~/anaconda3/envs/envname/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1103\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/envname/lib/python3.8/site-packages/torch_geometric/nn/dense/linear.py:109\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m    108\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\"\"\"\u001b[39;00m\n\u001b[0;32m--> 109\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(x, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/anaconda3/envs/envname/lib/python3.8/site-packages/torch/nn/functional.py:1848\u001b[0m, in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_variadic(\u001b[39minput\u001b[39m, weight, bias):\n\u001b[1;32m   1847\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(linear, (\u001b[39minput\u001b[39m, weight, bias), \u001b[39minput\u001b[39m, weight, bias\u001b[39m=\u001b[39mbias)\n\u001b[0;32m-> 1848\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, weight, bias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1690x81 and 27x216)"
     ]
    }
   ],
   "source": [
    "print('Starting training on a single gpu...')\n",
    "mode = 'gpu'\n",
    "train(0, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
